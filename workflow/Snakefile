import datetime
import re
import os
import pandas as pd

configfile: "config/config.yaml"

wildcard_constraints:
  chr = "chr[0-9X]{1,2}"

max_time = 240

def get_mem_mb(wildcards, threads):
    return threads * 3420

def get_permute_time(wildcards, threads):
    return min(max_time, 0.25*((int(wildcards.draws)/300)*3600)/int(threads))

metadata_daf = pd.read_csv('resources/gwas/metadata/metadata.tsv', sep = '\t')

preprocessed_gwas = ["ra",
                     "sle",
                     "crohns",
                     "uc-delange",
                     "t1d-cooper",
                     ]

traits = ["asthma-ex", # DONE
"osteo-ex", # DONE
"hypothy",
"leio-ex",
"crohns",
"cardiomyo-ex",
"derm-ecz",
"endomet-ex",
"ibs-ex",
"ra",
"sle",
"t1d",
"uc-delange",
"glaucoma-ex",
"hyperchol-ex", # DONE
"cholelith-ex",
"md-ex",
"rhin-ex"]

include: 'rules/wildcard_constraints.smk'
include: 'rules/1000g_reference.smk'
include: 'rules/gwas.smk'
include: 'rules/sumher.smk'
include: 'rules/export.smk'

# Ancillary Python code
include: 'rules/python/export.py'

module gwas_pipeline:
    snakefile: github("twillis209/GWAS_tools", path = "workflow/Snakefile", branch = "snakemake")
    config: config["GWAS_tools"]

use rule * from gwas_pipeline

use rule rehead from gwas_pipeline as rehead with:
    input:
        "resources/gwas/{input_name}.tsv.gz"

use rule merge_liftovered_rows_with_summary_statistics from gwas_pipeline as merge_liftovered_rows_with_summary_statistics with:
    output:
        "results/processed_gwas/{input_name}.tsv.gz"
